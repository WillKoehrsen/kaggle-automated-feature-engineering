{
  "cells": [
    {
      "metadata": {
        "_uuid": "09687305fb0b4ea77f0cda2b6f4a6c619d1f3090"
      },
      "cell_type": "markdown",
      "source": "# Introduction: Automated Feature Engineering Basics\n\nIn this notebook, we will walk through applying automated feature engineering to the [Home Credit Default Risk dataset](https://www.kaggle.com/c/home-credit-default-risk) using the featuretools library. [Featuretools](https://docs.featuretools.com/) is an open-source Python package for automatically creating new features from multiple tables of structured, related data. It is ideal tool for problems such as the Home Credit Default Risk competition where there are several related tables that need to be combined into a single dataframe for training (and one for testing). \n\n## Feature Engineering\n\nThe objective of [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering) is to create new features (alos called explantory variables or predictors) to represent as much information from an entire dataset in one table.  Typically, this process is done by hand using pandas operations such as `groupby`, `agg`, or `merge` and can be very tedious. Moreover, manual feature engineering is limited both by human time constraints and imagination: we simply cannot conceive of every possible feature that will be useful. (For an example of using manual feature engineering, check out [part one](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [part two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) applied to this competition). The importance of creating the proper features cannot be overstated because a machine learning model can only learn from the data we give to it. Extracting as much information as possible from the available datasets is crucial to creating an effective solution.\n\n[Automated feature engineering](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219) aims to help the data scientist with the problem of feature creation by automatically building hundreds or thousands of new features from a dataset. Featuretools - the only library for automated feature engineering at the moment - will not replace the data scientist, but it will allow her to focus on more valuable parts of the machine learning pipeline, such as delivering robust models into production. \n\nHere we will touch on the concepts of automated feature engineering with featuretools and show how to implement it for the Home Credit Default Risk competition. We will stick to the basics so we can get the ideas down and then build upon this foundation in later work when we customize featuretools. We will work with a subset of the data because this is a computationally intensive job that is outside the capabilities of the Kaggle kernels. I took the work done in this notebook and ran the methods on the entire dataset with the results [available here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools). At the end of this notebook, we'll look at the features themselves, as well as the results of modeling with different combinations of hand designed and automatically built features. \n\nIf you are new to this competition, I suggest checking out [this post to get started](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). For a good take on why features are so important, here's a [blog post](https://www.featurelabs.com/blog/secret-to-data-science-success/) by one of the developers of Featuretools. "
    },
    {
      "metadata": {
        "_uuid": "883de7c414f552d9e5156f9b06bdc9eb2c855fac",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncomment and run if kernel does not already have featuretools\n# !pip install featuretools",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# matplotlit and seaborn for visualizations\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "677e7f162883b005cae0c75ea10b54cc024dd007"
      },
      "cell_type": "markdown",
      "source": "# Problem\n\nThe Home Credit Default Risk competition is a supervised classification machine learning task. The objective is to use historical financial and socioeconomic data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:\n\n* __Supervised__: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n* __Classification__: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n\n## Dataset\n\nThe data is provided by [Home Credit](http://www.homecredit.net/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. \n\nThere are 7 different data files:\n\n* __application_train/application_test__: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the `SK_ID_CURR`. The training application data comes with the `TARGET` with indicating 0: the loan was repaid and 1: the loan was not repaid. \n* __bureau__: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau and is identified by the `SK_ID_BUREAU`, Each loan in the application data can have multiple previous credits.\n* __bureau_balance__: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length. \n* __previous_application__: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`. \n* __POS_CASH_BALANCE__: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n* __credit_card_balance__: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n* __installments_payment__: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment. \n\nThe diagram below (provided by Home Credit) shows how the tables are related. This will be very useful when we need to define relationships in featuretools. \n\n![image](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n\n### Read in Data and Create Small Datasets\n\nWe will read in the full dataset, sort by the `SK_ID_CURR` and keep only the first 1000 rows to make the calculations feasible. Later we can convert to a script and run with the entire datasets."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read in the datasets and limit to the first 1000 rows (sorted by SK_ID_CURR) \n# This allows us to actually see the results in a reasonable amount of time! \napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv').sort_values('SK_ID_CURR').reset_index().loc[:1000, :].drop(columns = ['index'])\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv').sort_values('SK_ID_CURR').reset_index().loc[:1000, :].drop(columns = ['index'])\nbureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv').sort_values(['SK_ID_CURR', 'SK_ID_BUREAU']).reset_index().loc[:1000, :].drop(columns = ['index'])\nbureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv').sort_values('SK_ID_BUREAU').reset_index().loc[:1000, :].drop(columns = ['index'])\ncash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\ncredit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\nprevious = pd.read_csv('../input/home-credit-default-risk/previous_application.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])\ninstallments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv').sort_values(['SK_ID_CURR', 'SK_ID_PREV']).reset_index().loc[:1000, :].drop(columns = ['index'])",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26f06adcab271a64e8f5040663da4f409920c00c"
      },
      "cell_type": "markdown",
      "source": "We'll join the train and test set together but add a separate column identifying the set. This is important because we are going to want to apply the same exact procedures to each dataset. It's safest to just join them together and treat them as a single dataframe. \n\n(I'm not sure if this is allowing data leakage into the train set and if these feature creation operations should be applied separately. Any thoughts would be much appreciated!)"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "884ab37d4c693ee6acde07c3037d34757cc58b78"
      },
      "cell_type": "code",
      "source": "# Add identifying column\napp_train['set'] = 'train'\napp_test['set'] = 'test'\napp_test[\"TARGET\"] = np.nan\n\n# Append the dataframes\napp = app_train.append(app_test, ignore_index = True)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40f23e82b3ad9665ff910ce458ebf31774bcc234"
      },
      "cell_type": "markdown",
      "source": "# Featuretools Basics\n\n[Featuretools](https://docs.featuretools.com/#minute-quick-start) is an open-source Python library for automatically creating features out of a set of related tables using a technique called [deep feature synthesis](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf). Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.\n\nThere are a few concepts that we will cover along the way:\n\n* [Entities and EntitySets](https://docs.featuretools.com/loading_data/using_entitysets.html)\n* [Relationships between tables](https://docs.featuretools.com/loading_data/using_entitysets.html#adding-a-relationship)\n* [Feature primitives](https://docs.featuretools.com/automated_feature_engineering/primitives.html): aggregations and transformations\n* [Deep feature synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html)\n\n# Entities and Entitysets\n\nAn entity is simply a table or in Pandas, a `dataframe`. The observations are in the rows and the features in the columns. An entity in featuretools must have a unique index where none of the elements are duplicated.  Currently, only `app`, `bureau`, and `previous` have unique indices (`SK_ID_CURR`, `SK_ID_BUREAU`, and `SK_ID_PREV` respectively). For the other dataframes, we must pass in `make_index = True` and then specify the name of the index. Entities can also have time indices where each entry is identified by a unique time. (There are not datetimes in any of the data, but there are relative times, given in months or days, that we could consider treating as time variables).\n\nAn [EntitySet](https://docs.featuretools.com/loading_data/using_entitysets.html) is a collection of tables and the relationships between them. This can be thought of a data structute with its own methods and attributes. Using an EntitySet allows us to group together multiple tables and manipulate them much quicker than individual tables. \n\nFirst we'll make an empty entityset named clients to keep track of all the data."
    },
    {
      "metadata": {
        "_uuid": "7a818b6fc16a7e3888f8df1c6ae7a8a09ce4dd99",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Entity set with id applications\nes = ft.EntitySet(id = 'clients')",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4cc60929493dce32417f1b66dd8dcd05f7036e7a"
      },
      "cell_type": "markdown",
      "source": "Now we define each entity, or table of data. We need to pass in an index if the data has one or `make_index = True` if not. Featuretools will automatically infer the types of variables, but we can also change them if needed. For intstance, if we have a categorical variable that is represented as an integer we might want to let featuretools know the right type."
    },
    {
      "metadata": {
        "_uuid": "79bb9b52f7671bcb2ffa2dc9ec10caaa481261cc",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Entities with a unique index\nes = es.entity_from_dataframe(entity_id = 'app', dataframe = app, index = 'SK_ID_CURR')\n\nes = es.entity_from_dataframe(entity_id = 'bureau', dataframe = bureau, index = 'SK_ID_BUREAU')\n\nes = es.entity_from_dataframe(entity_id = 'previous', dataframe = previous, index = 'SK_ID_PREV')\n\n# Entities that do not have a unique index\nes = es.entity_from_dataframe(entity_id = 'bureau_balance', dataframe = bureau_balance, \n                              make_index = True, index = 'bureaubalance_index')\n\nes = es.entity_from_dataframe(entity_id = 'cash', dataframe = cash, \n                              make_index = True, index = 'cash_index')\n\nes = es.entity_from_dataframe(entity_id = 'installments', dataframe = installments,\n                              make_index = True, index = 'installments_index')\n\nes = es.entity_from_dataframe(entity_id = 'credit', dataframe = credit,\n                              make_index = True, index = 'credit_index')",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b20993fb7eacd42fde984dd7313edae86dd9727e"
      },
      "cell_type": "markdown",
      "source": "# Relationships\n\nRelationships are a fundamental concept not only in featuretools, but in any relational database. The best way to think of a one-to-many relationship is with the analogy of parent-to-child. A parent is a single individual, but can have mutliple children. The children can then have multiple children of their own. In a _parent table_, each individual has a single row. Each individual in the parent table can have multiple rows in the _child table_. \n\nAs an example, the `app` dataframe has one row for each client  (`SK_ID_CURR`) while the `bureau` dataframe has multiple previous loans (`SK_ID_PREV`) for each parent (`SK_ID_CURR`). Therefore, the `bureau` dataframe is the child of the `app` dataframe. The `bureau` dataframe in turn is the parent of `bureau_balance` because each loan has one row in `bureau` but multiple monthly records in `bureau_balance`. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e998f79f4cf23b5efb5da7c349c8437136e4b156",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Parent: app, Parent Variable: SK_ID_CURR\\n\\n', app.iloc[:, 111:115].head())\nprint('\\nChild: bureau, Child Variable: SK_ID_CURR\\n\\n', bureau.iloc[10:30, :4].head())",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9cec78d486c6ec76e5ef5753529b9592b377ac5e"
      },
      "cell_type": "markdown",
      "source": "The `SK_ID_CURR` \"100002\" has one row in the parent table and multiple rows in the child. \n\nTwo tables are linked via a shared variable. The `app` and `bureau` dataframe are linked by the `SK_ID_CURR` variable while the `bureau` and `bureau_balance` dataframes are linked with the `SK_ID_BUREAU`. Defining the relationships is relatively straightforward, and the diagram provided by the competition is helpful for seeing the relationships. For each relationship, we need to specify the parent variable and the child variable. Altogether, there are a total of 6 relationships between the tables. Below we specify all six relationships and then add them to the EntitySet."
    },
    {
      "metadata": {
        "_uuid": "9808a64142c2f29cfe41301baed9e3f598174586",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Relationship between app and bureau\nr_app_bureau = ft.Relationship(es['app']['SK_ID_CURR'], es['bureau']['SK_ID_CURR'])\n\n# Relationship between bureau and bureau balance\nr_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n\n# Relationship between current app and previous apps\nr_app_previous = ft.Relationship(es['app']['SK_ID_CURR'], es['previous']['SK_ID_CURR'])\n\n# Relationships between previous apps and cash, installments, and credit\nr_previous_cash = ft.Relationship(es['previous']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\nr_previous_installments = ft.Relationship(es['previous']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\nr_previous_credit = ft.Relationship(es['previous']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72a31c3df020fc6402df204f6c66df7d99c3ee69",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Add in the defined relationships\nes = es.add_relationships([r_app_bureau, r_bureau_balance, r_app_previous,\n                           r_previous_cash, r_previous_installments, r_previous_credit])\n# Print out the EntitySet\nes",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7affcac0cf0a403966610a44cd9e6bf41be18754"
      },
      "cell_type": "markdown",
      "source": "Slightly advanced note: we need to be careful to not create a [diamond graph](https://en.wikipedia.org/wiki/Diamond_graph) where there are multiple paths from a parent to a child. If we directly link `app` and `cash` via `SK_ID_CURR`; `previous` and `cash` via `SK_ID_PREV`; and `app` and `previous` via `SK_ID_CURR`, then we have created two paths from `app` to `cash`. This results in ambiguity, so the approach we have to take instead is to link `app` to `cash` through `previous`. We establish a relationship between `previous` (the parent) and `cash` (the child) using `SK_ID_PREV`. Then we establish a relationship between `app` (the parent) and `previous` (now the child) using `SK_ID_CURR`. Then featuretools will be able to create features on `app` derived from both `previous` and `cash` by stacking multiple primitives. "
    },
    {
      "metadata": {
        "_uuid": "4ce410c7730e58f7f593350dc36f6eebd8f78c8a"
      },
      "cell_type": "markdown",
      "source": "All entities in the entity can be related to each other. In theory this allows us to calculate features for any of the entities, but in practice, we will only calculate features for the `app` dataframe since that will be used for training/testing. "
    },
    {
      "metadata": {
        "_uuid": "1af4e00532ef1a34bf962e624c34c1e87d51b01b"
      },
      "cell_type": "markdown",
      "source": "# Feature Primitives\n\nA [feature primitive](https://docs.featuretools.com/automated_feature_engineering/primitives.html) is an operation applied to a table or a set of tables to create a feature. These represent simple calculations, many of which we already use in manual feature engineering, that can be stacked on top of each other to create complex features. Feature primitives fall into two categories:\n\n* __Aggregation__: function that groups together child datapoints for each parent and then calculates a statistic such as mean, min, max, or standard deviation. An example is calculating the maximum previous loan amount for each client. An aggregation works across multiple tables using relationships between tables.\n* __Transformation__: an operation applied to one or more columns in a single table. An example would be taking the absolute value of a column, or finding the difference between two columns in one table.\n\nA list of the available features primitives in featuretools can be viewed below."
    },
    {
      "metadata": {
        "_uuid": "e1d874fb3bf2d1f251fc13c7ee11ded80efe6015",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# List the primitives in a dataframe\nprimitives = ft.list_primitives()\npd.options.display.max_colwidth = 100\nprimitives[primitives['type'] == 'aggregation'].head(10)",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1057d6584542d8e937f509f73afc0597bf9429e5",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "primitives[primitives['type'] == 'transform'].head(10)",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b21ecdae1aea0c7179257adb8ad329adb1418f81"
      },
      "cell_type": "markdown",
      "source": "# Deep Feature Synthesis\n\nDeep Feature Synthesis (DFS) is the process featuretools uses to make new features. DFS stacks feature primitives to form features with a \"depth\" equal to the number of primitives. For example, if we take the maximum value of a client's previous loans (say `MAX(previous.loan_amount)`), that is a \"deep feature\" with a depth of 1. To create a feature with a depth of two, we could stack primitives by taking the maximum value of a client's average montly payments per previous loan (such as `MAX(previous(MEAN(installments.payment)))`). The [original paper on automated feature engineering using deep feature synthesis](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf) is worth a read. \n\nTo perform DFS in featuretools, we use the `dfs`  function passing it an `entityset`, the `target_entity` (where we want to make the features), the `agg_primitives` to use, the `trans_primitives` to use and the `max_depth` of the features. Here we will use the default aggregation and transformation primitives,  a max depth of 2, and calculate primitives for the `app` entity. Because this process is computationally expensive, we can run the function using `features_only = True` to return only a list of the features and not calculate the features themselves. This can be useful to look at the resulting features before starting an extended computation."
    },
    {
      "metadata": {
        "_uuid": "674fed1008ae0930f8f5d8ebf92995df0a8800b9"
      },
      "cell_type": "markdown",
      "source": "### DFS with Default Primitives"
    },
    {
      "metadata": {
        "_uuid": "506cbdcc600ddcb10e4a7d427230c025783a217f",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Default primitives from featuretools\ndefault_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\ndefault_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\", \"haversine\", \"numwords\", \"characters\"]\n\n# DFS with specified primitives\nfeature_names = ft.dfs(entityset = es, target_entity = 'app',\n                       trans_primitives = default_trans_primitives,\n                       agg_primitives=default_agg_primitives, \n                       max_depth = 2, features_only=True)\n\nprint('%d Total Features' % len(feature_names))",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7853ca17b27f520c2d149a4f09f811de053660ee"
      },
      "cell_type": "markdown",
      "source": "If you are interested in running this call on the entire dataset and making the features, I wrote a script [for that here](https://www.kaggle.com/willkoehrsen/feature-engineering-using-feature-tools). Unfortunately, this will not run in a Kaggle kernel due to the computational expense of the operation. Using a computer with 64GB of ram, this function call took around 24 hours (I don't think I'm technically breaking the rules of my university's high powered computing center). I have made the entire dataset available [here](https://www.kaggle.com/willkoehrsen/home-credit-default-risk-feature-tools/data) in the file called `feature_matrix.csv`. \n\nTo generate a subset of the features, run the code cell below."
    },
    {
      "metadata": {
        "_uuid": "25e276da46d8159e1b6dda20ee16c234ec550777",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# DFS with default primitives\nfeature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app',\n                                       trans_primitives = default_trans_primitives,\n                                       agg_primitives=default_agg_primitives, \n                                        max_depth = 2, features_only=False, verbose = True)\n\npd.options.display.max_columns = 1700\nfeature_matrix.head(10)",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "343b754d4c60bba9b6a58dff0dc124a606054c98",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "feature_names[-20:]",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "917494ed4fde4aaa6babb7003ffc1cf76d892014"
      },
      "cell_type": "markdown",
      "source": "### DFS with Selected Aggregation Primitives\n\nWith featuretools, we were able to go from 121 original features to almost 1700 in a few lines of code.  When I did feature engineering by hand, it took about 12 hours to create a comparable size dataset. However, while we get a lot of features in featuretools, this function call is not very well-informed. We simply used the default aggregations without thinking about which ones are \"important\" for the problem. We end up with a lot of features, but they are probably not all relevant to the problem. Too many irrelevant features can decrease performance by drowning out the important features (related to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality))\n\nThe next call we make will specify a smaller set of features. We still are not using much domain knowledge, but this feature set will be more manageable. The next step from here is improving the features we actually build and performing feature selection."
    },
    {
      "metadata": {
        "_uuid": "1ad305fef7a4b4512c653f1490a43dd3e50c024e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Specify the aggregation primitives\nfeature_matrix_spec, feature_names_spec = ft.dfs(entityset = es, target_entity = 'app',  \n                                                 agg_primitives = ['sum', 'count', 'min', 'max', 'mean', 'mode'], \n                                                 max_depth = 2, features_only = False, verbose = True)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dbbe6b810c1e644118d68ae82445d84b250494f3"
      },
      "cell_type": "markdown",
      "source": "That \"only\" gives us 884 features (and takes about 12 hours to run on the complete dataset). "
    },
    {
      "metadata": {
        "_uuid": "217f8035c1e374db881b34f21649384e29db3c59",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pd.options.display.max_columns = 1000\nfeature_matrix_spec.head(10)",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "174deca79acf7ba32254f46bdecf8f976f8fca55"
      },
      "cell_type": "markdown",
      "source": "## Notes on Basic Implementation\n\nThese calls represent only a [small fraction of the ability of featuretools](https://docs.featuretools.com/guides/tuning_dfs.html). We did not specify the variable types when creating entities, did not use the relative time variables, and didn't touch on [custom primitives](https://docs.featuretools.com/guides/advanced_custom_primitives.html) or seed features or interesting values! Nonetheless, in this notebook, we were able to learn the basic foundations which will allow us to more effective use the tool as we learn how it works.  Now, let's take a look at some of the features we have built and modeling results."
    },
    {
      "metadata": {
        "_uuid": "233ac6f64ed0da5ff6e51c4cb50570e810fdc8a9"
      },
      "cell_type": "markdown",
      "source": "# Results\n\nTo determine whether our basic implementation of featuretools was useful, we can look at several results:\n\n* Cross validation scores and public leaderboard scores using several different sets of features.\n* Correlations: both between the features and the `TARGET`, and between features themselves\n* Feature importances: determined by a gradient boosting machine model\n"
    },
    {
      "metadata": {
        "_uuid": "50c39b18d9a47ec17a8b2d639bdb4ab8a7e6ada5"
      },
      "cell_type": "markdown",
      "source": "## Feature Performance Experiments\n\nTo compare a number of different feature sets for the machine learning task, I set up several experiments.. In order to isolate the effect of the features, the same model was used to test a number of different feature sets. The model (which can be viewed in the appendix) is a basic LightGBM algorithm using 5-fold cross validation for training and evaluation. First, we establish a control dataset, and then we carry out a series of experiments and present the results.\n\n* Control: using only data from the `application` dataset\n* Test One: manual feature engineering using only the `application`, `bureau` and `bureau_balance` data\n* Test Two: manual feature engineering using all datasets\n* Test Three: featuretools default features (in the `feature_matrix`)\n* Test Four: featuretools specified features (in the `feature_matrix_spec`)\n* Test Five: featuretools specified features combined with manual feature engineering \n\nThe number of features is after one-hot encoding, the validation receiver operating characteristic area under the curve (ROC AUC) is calculated using 5-fold cross validation, the test ROC AUC is from the public leaderboard, and the time spent designing is my best estimate of how long it took to make the dataset! \n\n| Test    | Number of Features | Validation ROC AUC | Test ROC AUC | Time Spent |\n|---------|--------------------|--------------------|--------------|--------|\n| Control | 241                |           0.760         |     0.745         |       0.25 hours  |\n| One     | 421                |       0.766             |      0.757        |        8 hours        |\n| Two     |      1465             |          0.785          |         0.783     |                 12 hours |\n| Three   | 1803               |      0.784              |       0.777       |               1 hour\n| Four    | 1156               |         0.786           |        0.779      |                 1.25 hours |\n| Five    |  1624                  |           0. 787        |      0.782        |                    13.25 hours |\n\n\nIt's hard to say which set is exactly the best (although I trust the cross validation scores more than the public leaderboard) but there are huge discrepancies is the time for development. The specified featuretools dataset was able to achieve nearly the same performance as the hand engineered features on the test set with 8% of the time invested. It's clear that featuretools delivered value on this problem, but it still did not leave us without a job. The vital role of the data scientist now comes down to choosing the correct set of primitives and selecting the best features from among all the candidates. "
    },
    {
      "metadata": {
        "_uuid": "8e474c117265975b8b2adc9960a3cb53a952bb2a"
      },
      "cell_type": "markdown",
      "source": "## Correlations\n\nNext we can look at correlations within the data. When we look at correlations with the target, we need to be careful about the [multiple comparisons problem](https://towardsdatascience.com/the-multiple-comparisons-problem-e5573e8b9578): if we make a ton of features, some are likely to be correlated with the target simply because of random noise. Using correlations is fine as a first approximation for identifying \"good features\", but it is not a rigorous feature selection method.  \n\nAlso, based on examining some of the features, it seems there might be issues with [collinearity between features](https://en.wikipedia.org/wiki/Multicollinearity) made by featuretools. Features that are highly correlated with one another can diminish interpretability and generalization performance on the test set. In an ideal scenario, we would have a set of independent features, but that rarely occurs in practice. If there are very highly correlated varibables, we might want to think about removing some of them.\n\nFor the correlations, we will focus on the `feature_matrix_spec`, the features we made by specifying the primitives. The same analysis could be applied to the default feature set. These correlations were calculated using the entire training section of the feature matrix."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95790c3955a004148f905b9d15103b45478f8dd3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "correlations = pd.read_csv('../input/home-credit-default-risk-feature-tools/correlations_spec.csv', index_col = 0)\ncorrelations.index.name = 'Variable'\ncorrelations.head()",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "24762e0bc7823bdd8ddb4ee2466a16a55ef42121"
      },
      "cell_type": "markdown",
      "source": "### Correlations with the Target"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8cb4ecdfc6861f7578c1fbf7f0e8cd5053dd30ed",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "correlations_target = correlations.sort_values('TARGET')['TARGET']\n# Most negative correlations\ncorrelations_target.head()",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "899efb220f79403a5b87b4dbd7000ef59206276d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Most positive correlations\ncorrelations_target.dropna().tail()",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "155b701ec092166e87aa6c239404a0de982a7364"
      },
      "cell_type": "markdown",
      "source": "Several of the features created by featuretools are among the most correlated with the `TARGET` (in terms of absolute magnitude). However, that does not mean they are necessarily \"important\". \n\n"
    },
    {
      "metadata": {
        "_uuid": "c7f094ead5751d56c88a5b11fc583a54c339aa25"
      },
      "cell_type": "markdown",
      "source": "### Visualize Distribution of Correlated Variables\n\nOne way we can look at the resulting features and their relation to the target is with a kernel density estimate plot. This shows the distribution of a single variable, and can be thought of as a smoothed histogram. To show the effect of a categorical variable on the distribution of a numeric variable, we can color the plot by th value of the categorical variable. In the plot below, we show the distribution of two of the newly created features, colored by the value of the target. \n\nFirst, we read in some of the feature matrix using the `nrows` argument of pandas `read_csv` function. This ensures we will not read in the entire 2 GB file. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1566cc12871203607203250cbac5c24c46438819",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "features_sample = pd.read_csv('../input/home-credit-default-risk-feature-tools/feature_matrix.csv', nrows = 20000)\nfeatures_sample = features_sample[features_sample['set'] == 'train']\nfeatures_sample.head()",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "cc11308f86fecfc9ffc7cfecf8b6af2400864cb2"
      },
      "cell_type": "code",
      "source": "def kde_target_plot(df, feature):\n    \"\"\"Kernel density estimate plot of a feature colored\n    by value of the target.\"\"\"\n    \n    # Need to reset index for loc to workBU\n    df = df.reset_index()\n    plt.figure(figsize = (10, 6))\n    plt.style.use('fivethirtyeight')\n    \n    # plot repaid loans\n    sns.kdeplot(df.loc[df['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(df.loc[df['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of Feature by Target Value')\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    plt.show()",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52555d76f03d250db897891a4daebce7f78a0cfe",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "kde_target_plot(features_sample, feature = 'MAX(previous_app.MEAN(credit.CNT_DRAWINGS_ATM_CURRENT))')",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34fb546e68df5213366a27f51019a4ecb94b87cb"
      },
      "cell_type": "markdown",
      "source": "The correlation between this feature and the target is extremely weak and could be only noise. Trying to interpret this feature is difficult, but my best guess is: a client's maximum value of average number of atm drawings per month on previous credit card loans. (We are only using a sample of the features, so this might not be representative of the entire dataset)."
    },
    {
      "metadata": {
        "_uuid": "608ca2e956f240081242f81f634f381fdc6e0546"
      },
      "cell_type": "markdown",
      "source": "Another area to investigate is highly correlated features, known as collinear features. We can look for pairs of correlated features and potentially remove any above a threshold."
    },
    {
      "metadata": {
        "_uuid": "5e4ba3ea80b358a79361e727b7cb6e0856425012"
      },
      "cell_type": "markdown",
      "source": "#### Collinear Features"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "32b951da7e66f2107725806520ef0688093ff9bd"
      },
      "cell_type": "code",
      "source": "threshold = 0.9\n\ncorrelated_pairs = {}\n\n# Iterate through the columns\nfor col in correlations:\n    # Find correlations above the threshold\n    above_threshold_vars = [x for x in list(correlations.index[correlations[col] > threshold]) if x != col]\n    correlated_pairs[col] = above_threshold_vars",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53dddec6d2530e92eab7a7437d90711ab6a46a24",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "correlated_pairs['MEAN(credit.AMT_PAYMENT_TOTAL_CURRENT)']",
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16c8d0bd3af28fc06ef931faf5a9a00548e27590",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "correlations['MEAN(credit.AMT_PAYMENT_TOTAL_CURRENT)'].sort_values(ascending=False).head()",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77d60f382bdf9c5835c3b613037bc36224b32331",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.plot(features_sample['MEAN(credit.AMT_PAYMENT_TOTAL_CURRENT)'], features_sample['MEAN(previous_app.MEAN(credit.AMT_PAYMENT_CURRENT))'], 'bo')\nplt.title('Highly Correlated Features');",
      "execution_count": 67,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "11b2ae1bbf6d3ed6cfc4485099f406d3cf854b6d"
      },
      "cell_type": "markdown",
      "source": "These variables all have a 0.99 correlation with each other which is nearly perfectly positively linear. Including them all in the model is unnecessary because it would be encoding redundant information. We would probably want to remove some of these highly correlated variables in order to help the model learn and generalize better. "
    },
    {
      "metadata": {
        "_uuid": "4c8a795d5d857eac18c3545ad028ad726d6e84ee"
      },
      "cell_type": "markdown",
      "source": "## Feature Importances\n\nThe feature importances returned by a tree-based model [represent the reduction in impurity](https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) from including the feature in the model. While the absolute value of the importances can be difficult to interpret, looking at the relative value of the importances allows us to compare the relevance of features. Although we want to be careful about placing too much value on the feature importances, they can be a useful method for dimensionality reduction and understanding the model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bdbb2fc0e1dc26c05cdc9baf985ac33cc0ea2e2e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Read in the feature importances and sort with the most important at the top\nfi = pd.read_csv('../input/home-credit-default-risk-feature-tools/spec_feature_importances_ohe.csv', index_col = 0)\nfi = fi.sort_values('importance', ascending = False)\nfi.head(15)",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04205d9af1c4eb294ed4cd724fd56d2132c99ce1"
      },
      "cell_type": "markdown",
      "source": "The most important feature created by featuretools was the maximum number of days before current application that the client applied for a loan at another institution. (This feature is originally recorded as negative, so the maximum value would be closest to zero)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b24c6907d6e66c64cdd6d76565919280978c6ae",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "kde_target_plot(features_sample, feature = 'MAX(bureau.DAYS_CREDIT)')",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f17055e9cbea0fd57676ca8ef81b1da288fad1f8"
      },
      "cell_type": "markdown",
      "source": "We can calculate the number of top 100 features that were made by featuretools. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1950e3f6d273860387ddbf809ee115ba55afb3b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# List of the original features (after one-hot)\noriginal_features = list(pd.get_dummies(app).columns)\n\ncreated_features = []\n\n# Iterate through the top 100 features\nfor feature in fi['feature'][:100]:\n    if feature not in original_features:\n        created_features.append(feature)\n        \nprint('%d of the top 100 features were made by featuretools' % len(created_features))",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "343f4288c729eb6019f634fe33a114334447f50e"
      },
      "cell_type": "markdown",
      "source": "Let's write a short function to visualize the 15 most important features. "
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6440b790c4b9f8e8130956828e8efa790a56db40"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\n\ndef plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Parameters\n    --------\n        df : dataframe\n            feature importances. Must have the features in a column\n            called `features` and the importances in a column called `importance\n        \n    Return\n    -------\n        shows a plot of the 15 most importance features\n        \n        df : dataframe\n            feature importances sorted by importance (highest to lowest) \n            with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (14, 10))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec0734f23fe1fad249460f7836323c1783894ba1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fi = plot_feature_importances(fi)",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6499c4adfc22724cc9c1e9971f6bf63a2651ccc0"
      },
      "cell_type": "markdown",
      "source": "The most important feature created by featuretools was `MAX(bureau.DAYS_CREDIT)`. `DAYS_CREDIT` represents the number of days before the current application at Home Credit that the applicant applied for a loan at another credit institution. The maximum of this value (over the previous loans) is therefore represented by this feature. We also see several important features with a depth of two such as `MEAN(previous_app.MIN(installments.AMT_PAYMENT))` which is the average over a client's loans of the minimum value of previous credit application installment payments. \n\nFeature importances can be used for dimensionality reduction. They can also be used to help us better understand a problem. For example, we could use the most important features in order to concentrate on these aspects of a client when evaluating a potential loan. Let's look at the number of features with 0 importance which almost certainly can be removed from the featureset. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca081a35c518821c155755689334e469c702ce1e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('There are %d features with 0 importance' % sum(fi['importance'] == 0.0))",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d5aa0ab037e03ff4702677e685f38d98a06e581"
      },
      "cell_type": "markdown",
      "source": "## Remove Low Importance Features\n\nFeature selection is an entire topic by itself, but one thing we can do is remove any features that have only a single unique value or are all null. Featuretools has a default method for doing this available in the `selection` module."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ccb0272f9e0ab12757a7b73e56c845b7ebc5da4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from featuretools import selection\n\n# Remove features with only one unique value\nfeature_matrix2 = selection.remove_low_information_features(feature_matrix)\n\nprint('Removed %d features' % (feature_matrix.shape[1]- feature_matrix2.shape[1]))",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "feba310843ba81ea7e242cc2dc46eff3b6c61967"
      },
      "cell_type": "markdown",
      "source": "## Align Train and Test Sets\n\nWe also want to make sure the train and test sets have the same exact features. We can first one-hot encode the data (we'll have to do this anyway for our model) and then align the dataframes on the columns."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71fa2f4602f4a084fa745f4af81e2c6b45e8a5de",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Separate out the train and test sets\ntrain = feature_matrix2[feature_matrix2['set'] == 'train']\ntest = feature_matrix2[feature_matrix2['set'] == 'test']\n\n# One hot encoding\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\n# Align dataframes on the columns\ntrain, test = train.align(test, join = 'inner', axis = 1)\ntest = test.drop(columns = ['TARGET'])\n\nprint('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1187f0ab2c807387be97d32af225c7836c879839"
      },
      "cell_type": "markdown",
      "source": "Removing the low information features and aligning the dataframes has left us with 1689 features! Feature selection will certainly play an important role when using featuretools. "
    },
    {
      "metadata": {
        "_uuid": "b28d2f86b3b42283f18ce1a6b2b796263db78412"
      },
      "cell_type": "markdown",
      "source": " # Conclusions\n\nIn this notebook we went through a basic implementation of using automated feature engineering with featuretools for the Home Credit Default Risk dataset. Although we did not use the advanced functionality of featuretools, we still were able to create useful features that improved the model's performance in cross validation and on the test set. Moreover, automated feature engineering took a fraction of the time spent manual feature engineering while delivering comparable results. \n\n__Even the default set of features in featuretools was able to achieve similar performance to hand-engineered features in less than 10% of the time.__\n__Featuretools demonstrably adds value when included in a data scientist's toolbox.__\n\nThe next steps are to take advantage of the advanced functionality in featuretools combined with domain knowledge to create a more useful set of features. We will look explore [tuning featuretools in an upcoming notebook](https://www.kaggle.com/willkoehrsen/intro-to-tuning-automated-feature-engineering)!"
    },
    {
      "metadata": {
        "_uuid": "4d0e1bf154369fbf55d3e5bfaeed2e2f2fa1520b"
      },
      "cell_type": "markdown",
      "source": "## Appendix: GBM Model (Used Across Feature Sets)\n```python\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, boosting_type = 'goss',\n\t\t\t\t   objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics\n```"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "42d5e3c9244b30a8e858c22742202b815bb666b5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}